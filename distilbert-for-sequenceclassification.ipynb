{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Kaggle Competition : Disaster Tweets Classification.","metadata":{}},{"cell_type":"markdown","source":"The purpose of this competition is to classify tweets according to two classes: are the sequences of text talking about a real natural disaster or not? \n\nTo implement my solution, I will use the HuggingFace/TensorFlow frameworks and more specifically I will fine-tune a DistilBERT type of Transformer. DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERTâ€™s performances as measured on the GLUE language understanding benchmark.","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-10T17:44:05.030754Z","iopub.execute_input":"2021-12-10T17:44:05.031038Z","iopub.status.idle":"2021-12-10T17:44:05.041397Z","shell.execute_reply.started":"2021-12-10T17:44:05.031009Z","shell.execute_reply":"2021-12-10T17:44:05.040654Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"Firstly, let's import the Tokenizer that was used to pre-train the DistilBERT to ensure our sequences of text will be encoded appropriately for this model.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom transformers import DistilBertTokenizerFast\nfrom transformers import TFDistilBertForSequenceClassification","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:44:05.044452Z","iopub.execute_input":"2021-12-10T17:44:05.044771Z","iopub.status.idle":"2021-12-10T17:44:05.055759Z","shell.execute_reply.started":"2021-12-10T17:44:05.044740Z","shell.execute_reply":"2021-12-10T17:44:05.054762Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"Now let's load the CSV file into a Pandas dataframe and retain information from the 'text' column for our independant feature, and the 'target' column for our dependant feature.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/nlp-getting-started/train.csv')\ndf","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:44:05.177413Z","iopub.execute_input":"2021-12-10T17:44:05.177761Z","iopub.status.idle":"2021-12-10T17:44:05.214421Z","shell.execute_reply.started":"2021-12-10T17:44:05.177648Z","shell.execute_reply":"2021-12-10T17:44:05.213526Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"X = list(df['text'])\ny = list(df['target'])","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:44:05.216255Z","iopub.execute_input":"2021-12-10T17:44:05.216577Z","iopub.status.idle":"2021-12-10T17:44:05.222859Z","shell.execute_reply.started":"2021-12-10T17:44:05.216542Z","shell.execute_reply":"2021-12-10T17:44:05.222153Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"Let's generate our training and validation datasets using Scikit-Learn:","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.20, random_state = 0)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:44:05.224382Z","iopub.execute_input":"2021-12-10T17:44:05.225286Z","iopub.status.idle":"2021-12-10T17:44:05.238109Z","shell.execute_reply.started":"2021-12-10T17:44:05.225189Z","shell.execute_reply":"2021-12-10T17:44:05.237174Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"Let's instantiate our tokenizer to generate encodings for the training and validation datasets, using 'truncation' and 'padding' in order to make all sequences of equal length:","metadata":{}},{"cell_type":"code","source":"tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:44:05.241070Z","iopub.execute_input":"2021-12-10T17:44:05.241429Z","iopub.status.idle":"2021-12-10T17:44:06.463563Z","shell.execute_reply.started":"2021-12-10T17:44:05.241389Z","shell.execute_reply":"2021-12-10T17:44:06.462750Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"train_encodings = tokenizer(X_train,\n                            truncation=True,\n                            padding=True)\n\nval_encodings = tokenizer(X_val,\n                            truncation=True,\n                            padding=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:44:06.465229Z","iopub.execute_input":"2021-12-10T17:44:06.465657Z","iopub.status.idle":"2021-12-10T17:44:07.274047Z","shell.execute_reply.started":"2021-12-10T17:44:06.465613Z","shell.execute_reply":"2021-12-10T17:44:07.273291Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"Let's convert our datasets to 'tf.data.Dataset' format by combining the encodings and their labels:","metadata":{}},{"cell_type":"code","source":"train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train))\nval_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), y_val))","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:44:07.275560Z","iopub.execute_input":"2021-12-10T17:44:07.275805Z","iopub.status.idle":"2021-12-10T17:44:11.092649Z","shell.execute_reply.started":"2021-12-10T17:44:07.275772Z","shell.execute_reply":"2021-12-10T17:44:11.091873Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"Let's import the DistilBERT pretrained model with the following default configuration:\n\n( vocab_size = 30522, max_position_embeddings = 512, sinusoidal_pos_embds = False, n_layers = 6, n_heads = 12, dim = 768, hidden_dim = 3072, dropout = 0.1, attention_dropout = 0.1, activation = 'gelu', initializer_range = 0.02, qa_dropout = 0.1, seq_classif_dropout = 0.2, pad_token_id = 0**kwargs )","metadata":{}},{"cell_type":"markdown","source":"Let's compile our model for 2 classes and an 'Adam' optimizer:","metadata":{}},{"cell_type":"code","source":"model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:44:11.095908Z","iopub.execute_input":"2021-12-10T17:44:11.096116Z","iopub.status.idle":"2021-12-10T17:44:12.030282Z","shell.execute_reply.started":"2021-12-10T17:44:11.096092Z","shell.execute_reply":"2021-12-10T17:44:12.029546Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"#optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n\nfrom transformers.optimization_tf import AdamWeightDecay\noptimizer = AdamWeightDecay(\n            learning_rate=5e-5,\n            weight_decay_rate=0.0,\n            beta_1=0.9,\n            beta_2=0.999,\n            epsilon=1e-8,\n            exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"],\n            include_in_weight_decay=None,\n        )","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:44:12.031669Z","iopub.execute_input":"2021-12-10T17:44:12.033506Z","iopub.status.idle":"2021-12-10T17:44:12.039140Z","shell.execute_reply.started":"2021-12-10T17:44:12.033466Z","shell.execute_reply":"2021-12-10T17:44:12.038299Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:44:12.042039Z","iopub.execute_input":"2021-12-10T17:44:12.042239Z","iopub.status.idle":"2021-12-10T17:44:12.056561Z","shell.execute_reply.started":"2021-12-10T17:44:12.042216Z","shell.execute_reply":"2021-12-10T17:44:12.055782Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"model.fit(train_dataset.shuffle(100).batch(16),\n          epochs=1,\n          batch_size=16,\n          validation_data=val_dataset.shuffle(100).batch(16))","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:44:12.060571Z","iopub.execute_input":"2021-12-10T17:44:12.060845Z","iopub.status.idle":"2021-12-10T17:45:01.524242Z","shell.execute_reply.started":"2021-12-10T17:44:12.060809Z","shell.execute_reply":"2021-12-10T17:45:01.523492Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"Saving and loading our fine-tuned model:","metadata":{}},{"cell_type":"code","source":"model.save_pretrained(\"MSF_DistilBERT_CustomModel\")","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:45:01.525867Z","iopub.execute_input":"2021-12-10T17:45:01.526141Z","iopub.status.idle":"2021-12-10T17:45:02.228295Z","shell.execute_reply.started":"2021-12-10T17:45:01.526104Z","shell.execute_reply":"2021-12-10T17:45:02.227401Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"loaded_model = TFDistilBertForSequenceClassification.from_pretrained(\"MSF_DistilBERT_CustomModel\")","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:45:02.233542Z","iopub.execute_input":"2021-12-10T17:45:02.235831Z","iopub.status.idle":"2021-12-10T17:45:02.808058Z","shell.execute_reply.started":"2021-12-10T17:45:02.235785Z","shell.execute_reply":"2021-12-10T17:45:02.807251Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"Let's make an inference by tokenizing a test sentence and then passing it to our trained model. Let's apply 'softmax' to vizualize the probability for each class:","metadata":{}},{"cell_type":"code","source":"test_sentence = \"Terrible earthquake this morning, everyone was scared.\"","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:45:02.809564Z","iopub.execute_input":"2021-12-10T17:45:02.809933Z","iopub.status.idle":"2021-12-10T17:45:02.815097Z","shell.execute_reply.started":"2021-12-10T17:45:02.809893Z","shell.execute_reply":"2021-12-10T17:45:02.814333Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"predict_input = tokenizer.encode(test_sentence,\n                                 truncation=True,\n                                 padding=True,\n                                 return_tensors=\"tf\")","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:45:02.816611Z","iopub.execute_input":"2021-12-10T17:45:02.817666Z","iopub.status.idle":"2021-12-10T17:45:02.824628Z","shell.execute_reply.started":"2021-12-10T17:45:02.817629Z","shell.execute_reply":"2021-12-10T17:45:02.823793Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"tf_output = loaded_model.predict(predict_input)[0]","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:45:02.825844Z","iopub.execute_input":"2021-12-10T17:45:02.826226Z","iopub.status.idle":"2021-12-10T17:45:04.071991Z","shell.execute_reply.started":"2021-12-10T17:45:02.826187Z","shell.execute_reply":"2021-12-10T17:45:04.071170Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"tf_prediction = tf.nn.softmax(tf_output, axis=1).numpy()[0]\ntf_prediction","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:45:04.073426Z","iopub.execute_input":"2021-12-10T17:45:04.073714Z","iopub.status.idle":"2021-12-10T17:45:04.083577Z","shell.execute_reply.started":"2021-12-10T17:45:04.073677Z","shell.execute_reply":"2021-12-10T17:45:04.082412Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"Now let's evaluate our model with the test set. We will have to tokenize each sequence of text and pass it to the 'predict' method. Then we will obtain the probabilities for each class by applying softmax, and finally return the index of the most likely class by using 'argmax':","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('../input/nlp-getting-started/test.csv')\ndata = list(df['text'])","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:45:04.085352Z","iopub.execute_input":"2021-12-10T17:45:04.085698Z","iopub.status.idle":"2021-12-10T17:45:04.104205Z","shell.execute_reply.started":"2021-12-10T17:45:04.085654Z","shell.execute_reply":"2021-12-10T17:45:04.103476Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"results = []\nfor txt in data:\n    tokenized_input = tokenizer.encode(txt,\n                                 truncation=True,\n                                 padding=True,\n                                 return_tensors=\"tf\")\n    preds = loaded_model.predict(tokenized_input)\n    proba = tf.math.softmax(preds.logits, axis=-1)\n    label = proba.numpy()\n    results.append(label.argmax())","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:45:04.105615Z","iopub.execute_input":"2021-12-10T17:45:04.105912Z","iopub.status.idle":"2021-12-10T17:47:38.359981Z","shell.execute_reply.started":"2021-12-10T17:45:04.105881Z","shell.execute_reply":"2021-12-10T17:47:38.359212Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"Let's concatenate the list of IDs and the matching list of predicted class in a Dataframe, then write it all to our final CSV submission file:","metadata":{}},{"cell_type":"code","source":"sub = pd.DataFrame(np.column_stack((list(df['id']), results)), columns=[\"id\", \"target\"])","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:47:38.361283Z","iopub.execute_input":"2021-12-10T17:47:38.361715Z","iopub.status.idle":"2021-12-10T17:47:38.369368Z","shell.execute_reply.started":"2021-12-10T17:47:38.361676Z","shell.execute_reply":"2021-12-10T17:47:38.368591Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"sub.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T17:47:38.370655Z","iopub.execute_input":"2021-12-10T17:47:38.371201Z","iopub.status.idle":"2021-12-10T17:47:38.389289Z","shell.execute_reply.started":"2021-12-10T17:47:38.371151Z","shell.execute_reply":"2021-12-10T17:47:38.388537Z"},"trusted":true},"execution_count":42,"outputs":[]}]}